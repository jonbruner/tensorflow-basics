{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Save\n",
    "\n",
    "This notebook, along with its companion notebook [`load.ipynb`](load.ipynb), illustrates the process of saving a neural network model and its weights using TensorFlow, then loading them to be run as part of a separate program.\n",
    "\n",
    "Some details of saving and loading are non-obvious, and require a clear understanding of TensorFlow's abstraction. As a programmer of perhaps intermediate skills, it took me a while to grasp all of this, but once you've figured it out, saving and loading is very straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "The code in the block below is mostly copied straight from TensorFlow's [*Deep MNIST for Experts*](https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html) tutorial, which is a good introduction to convolutional neural networks. This code trains a CNN to classify handwritten images using the MNIST dataset. Read the tutorial if you're interested in an introduction to convolution and/or TensorFlow; I'll reproduce the tutorial code as compactly as possible in order to move on to the saving bit.\n",
    "\n",
    "I've made just a few modifications to the code below from what's published in TensorFlow's tutorial: I'm only running 1,000 iterations of the training step to save time (instead of 20,000) because accuracy isn't essential here. And I've added `name` flags to three variables: `x`, `keep_prob`, and `y_conv`. The addition of names will make sense later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name=\"input\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2, name=\"output\")\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# To make this run faster, we'll only run 1,000 iterations of the training process.\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Let's take a moment to review how this model can be used to classify a new image. We'll begin with an arbitrary image from the MNIST validation set and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_a = mnist.validation.images[154]\n",
    "plt.imshow(image_a.reshape([28, 28]), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this image is the digit 9. Let's run it through the model to see if the model agrees that it's a 9.\n",
    "\n",
    "The output of the model is the variable `y_conv`. To evaluate `y_conv`, we need to call `sess.run`, and we tell TensorFlow what to use as the model's inputs by feeding the inputs as a `feed_dict`.\n",
    "\n",
    "The MNIST convlutional neural network requires two inputs: `x`, which is the image we're trying to classify, and `keep_prob`, which is used in the dropout step in training, but will be 1 when we run the model here.\n",
    "\n",
    "The output of the model, `y_conv`, is a tensor consisting of a 1 x 10 list of probabilities; for any digit `n` between 0 and 9, `y_conv[n]` is the model's estimate of the probability that the image we fed in as `x` represents `n`. The last line below prints the index of the higest value in the `y_conv` list, which should be the digit we're looking for. We begin by reshaping `image_a` into a 1 x 784 matrix, which is the shape that `x` anticipates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_a = image_a.reshape([1, 784])\n",
    "result = sess.run(y_conv, feed_dict={x:image_a, keep_prob:1})\n",
    "print(result)\n",
    "print(sess.run(tf.argmax(result, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will prodce a slightly different result each time it's trained, but I got what I'm expecting: the model correctly says this handwritten digit is a 9, with roughly 82% certainty. The next closest guess is that it's a 7, with 17% certainty. That makes sense; this digit does indeed look a bit like a 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "So, we've trained this model and we've seen how to run it. Now we want to save it so it can be reinstated in another program and used to classify images there.\n",
    "\n",
    "The key realization at this point is that the TensorFlow model and its weights are different things, and it's possible to export one or the other. We want to export both. The model, or graph definition, defines the variables, the computational steps, and the links between them. The weights are the model values that we developed by training the model a few minutes ago.\n",
    "\n",
    "To save the model, we use a TensorFlow `saver`. The code below saves the model into our current working directory as two files: `saved_mnist_cnn.ckpt`, which contains the weights, and `saved_mnist_cnn.ckpt.meta`, which contains the graph definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"saved_mnist_cnn.ckpt\")\n",
    "print(\"Model saved to %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move on to part 2\n",
    "\n",
    "To demonstrate loading, we'll move on to a new file: [2: Load](load.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
